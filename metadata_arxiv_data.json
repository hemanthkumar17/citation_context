{"arxiv_data/Using large language models for (de-)formalization and natural argumentation exercises for beginner's students/2304.06186v1.Using_large_language_models_for__de__formalization_and_natural_argumentation_exercises_for_beginner_s_students.cermxml": {"ref6": "arxiv_data/Number Theory and Axiomatic Geometry in the Diproche System", "ref12": null, "ref8": null, "ref4": null, "ref7": "arxiv_data/Improving the Diproche CNL through autoformalization via GPT-3", "ref9": "arxiv_data/Using Automated Theorem Provers for Mistake Diagnosis in the Didactics of Mathematics"}, "arxiv_data/Are Transformers universal approximators of sequence-to-sequence functions?/1912.10077v2.Are_Transformers_universal_approximators_of_sequence_to_sequence_functions_.cermxml": {"ref23": "arxiv_data/Attention Is All You Need", "ref18": null, "ref13": "arxiv_data/RoBERTa: A Robustly Optimized BERT Pretraining Approach", "ref27": "arxiv_data/XLNet: Generalized Autoregressive Pretraining for Language Understanding", "ref7": "arxiv_data/BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "ref16": "arxiv_data/Distributed Representations of Words and Phrases and their Compositionality", "ref10": null, "ref24": "arxiv_data/Analyzing the Structure of Attention in a Transformer Language Model", "ref3": "arxiv_data/What Does BERT Look At? An Analysis of BERT's Attention", "ref4": "arxiv_data/Visualizing and Measuring the Geometry of BERT", "ref5": null, "ref12": "arxiv_data/ResNet with one-neuron hidden layers is a Universal Approximator", "ref14": "arxiv_data/The Expressive Power of Neural Networks: A View from the Width", "ref9": "arxiv_data/Approximating Continuous Functions by ReLU Nets of Minimal Width", "ref11": "arxiv_data/Depthwise Separable Convolutions for Neural Machine Translation", "ref6": "arxiv_data/Universal Transformers", "ref15": null, "ref8": null, "ref1": null, "ref2": null, "ref22": null, "ref20": null, "ref25": "arxiv_data/A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "ref19": null, "ref26": "arxiv_data/Pay Less Attention with Lightweight and Dynamic Convolutions"}, "arxiv_data/Large Language Models are not Models of Natural Language: they are Corpus Models/2112.07055v2.Large_Language_Models_are_not_Models_of_Natural_Language__they_are_Corpus_Models.cermxml": {"ref1": null, "ref2": null, "ref3": null, "ref4": null, "ref5": null, "ref6": null, "ref7": null, "ref8": null, "ref9": null, "ref10": null, "ref11": null, "ref12": null, "ref13": null, "ref14": null, "ref15": null, "ref16": null, "ref18": null, "ref19": null, "ref20": null, "ref21": null, "ref23": null, "ref24": null, "ref25": null, "ref26": null, "ref27": null, "ref28": null, "ref29": null, "ref30": null, "ref31": null, "ref32": null, "ref33": null, "ref34": "arxiv_data/Are Transformers universal approximators of sequence-to-sequence functions?", "ref35": null, "ref36": null, "ref37": "arxiv_data/Assessing BERT's Syntactic Abilities", "ref38": null, "ref39": "arxiv_data/A Primer in BERTology: What we know about how BERT works", "ref40": null, "ref41": null, "ref42": null, "ref43": null, "ref44": null, "ref45": null, "ref46": null, "ref47": "arxiv_data/Measuring Coding Challenge Competence With APPS", "ref48": null, "ref49": null, "ref50": "arxiv_data/The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "ref51": null, "ref52": "arxiv_data/Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks", "ref53": null, "ref54": null, "ref55": null, "ref56": null, "ref57": null, "ref58": null}, "arxiv_data/Assessing BERT's Syntactic Abilities/1901.05287v1.Assessing_BERT_s_Syntactic_Abilities.cermxml": {"ref1": "arxiv_data/Colorless green recurrent networks dream hierarchically"}, "arxiv_data/On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages/2204.09653v1.On_the_Transferability_of_Pre_trained_Language_Models_for_Low_Resource_Programming_Languages.cermxml": {"ref11": "arxiv_data/BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "ref30": "arxiv_data/RoBERTa: A Robustly Optimized BERT Pretraining Approach", "ref12": null, "ref13": null, "ref31": null, "ref33": "arxiv_data/Impact of Evaluation Methodologies on Code Summarization", "ref4": "arxiv_data/Program Synthesis with Large Language Models", "ref8": "arxiv_data/An Empirical Study on the Usage of Transformer Models for Code Completion", "ref3": "arxiv_data/Multilingual training for Software Engineering", "ref17": "arxiv_data/CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "ref1": null, "ref15": null, "ref19": null, "ref22": "arxiv_data/Learning and Evaluating Contextual Embedding of Source Code", "ref36": null, "ref37": null, "ref48": null, "ref9": null, "ref28": null, "ref43": "arxiv_data/Improving Automatic Source Code Summarization via Deep Reinforcement Learning", "ref45": null, "ref49": null, "ref21": null, "ref23": "arxiv_data/SCELMo: Source Code Embeddings from Language Models", "ref47": "arxiv_data/Incorporating External Knowledge through Pre-training for Natural Language to Code Generation", "ref7": "arxiv_data/Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations", "ref10": null, "ref41": "arxiv_data/IntelliCode Compose: Code Generation Using Transformer", "ref24": null, "ref2": "arxiv_data/Unified Pre-training for Program Understanding and Generation", "ref39": "arxiv_data/Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "ref32": "arxiv_data/Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks", "ref46": null, "ref40": "arxiv_data/Unsupervised Translation of Programming Languages", "ref44": null, "ref18": null, "ref27": null, "ref25": null, "ref16": "arxiv_data/Better Word Embeddings by Disentangling Contextual n-Gram Information", "ref20": null, "ref26": null, "ref34": null, "ref5": null, "ref35": null, "ref6": null, "ref38": null, "ref42": null, "ref14": null, "ref29": "arxiv_data/Text Summarization with Pretrained Encoders"}, "arxiv_data/Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models/2205.06130v1.Multi_Task_Learning_For_Zero_Shot_Performance_Prediction_of_Multilingual_Models.cermxml": {"ref13": null, "ref10": null, "ref34": null, "ref29": null, "ref4": "arxiv_data/Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond", "ref42": "arxiv_data/Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT", "ref20": "arxiv_data/Cross-Lingual Ability of Multilingual BERT: An Empirical Study", "ref21": "arxiv_data/From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers", "ref3": "arxiv_data/On the Cross-lingual Transferability of Monolingual Representations", "ref7": "arxiv_data/Entity Linking in 100 Languages", "ref35": null, "ref16": null, "ref28": null, "ref45": null, "ref44": null, "ref12": "arxiv_data/Cold-start recommendations in Collective Matrix Factorization", "ref6": null, "ref14": null, "ref38": "arxiv_data/Predicting the Performance of Multilingual NLP Models", "ref43": "arxiv_data/Towards More Fine-grained and Reliable NLP Performance Prediction", "ref36": "arxiv_data/How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models", "ref22": null, "ref39": "arxiv_data/Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition", "ref40": null, "ref27": null, "ref37": "arxiv_data/A Corpus for Multilingual Document Classification in Eight Languages", "ref11": null, "ref30": "arxiv_data/Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach", "ref31": null, "ref15": null, "ref25": null, "ref8": null, "ref2": null, "ref18": null, "ref26": null, "ref1": null, "ref32": null, "ref41": "arxiv_data/Deep Kernel Learning", "ref17": null, "ref33": null, "ref23": null, "ref9": "arxiv_data/TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages", "ref19": "arxiv_data/The State and Fate of Linguistic Diversity and Inclusion in the NLP World"}, "arxiv_data/Measuring Coding Challenge Competence With APPS/2105.09938v3.Measuring_Coding_Challenge_Competence_With_APPS.cermxml": {"ref35": null, "ref36": null, "ref37": "arxiv_data/RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers", "ref15": "arxiv_data/Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning", "ref3": null, "ref42": null, "ref20": null, "ref21": null, "ref12": "arxiv_data/Aligning AI With Shared Human Values", "ref13": "arxiv_data/Measuring Massive Multitask Language Understanding", "ref14": "arxiv_data/Measuring Mathematical Problem Solving With the MATH Dataset", "ref26": "arxiv_data/Generative Language Modeling for Automated Theorem Proving", "ref28": null, "ref29": null, "ref38": "arxiv_data/Graph-based, Self-Supervised Program Repair from Diagnostic Feedback", "ref34": "arxiv_data/Attention Is All You Need", "ref18": "arxiv_data/Unsupervised Translation of Programming Languages", "ref17": null, "ref25": null, "ref2": null, "ref11": null, "ref6": "arxiv_data/Making Neural Programming Architectures Generalize via Recursion", "ref32": null, "ref27": null, "ref5": null, "ref4": null, "ref9": "arxiv_data/The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "ref22": "arxiv_data/Decoupled Weight Decay Regularization", "ref16": "arxiv_data/Mapping Language to Code in Programmatic Context", "ref23": null, "ref30": null, "ref39": "arxiv_data/Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task", "ref19": "arxiv_data/Latent Predictor Networks for Code Generation", "ref24": null}, "arxiv_data/Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks/2010.12621v1.Learning_to_Execute_Programs_with_Instruction_Pointer_Attention_Graph_Neural_Networks.cermxml": {"ref5": "arxiv_data/Generative Code Modeling with Graphs", "ref13": null, "ref2": "arxiv_data/Learning to Represent Programs with Heterogeneous Graphs", "ref25": "arxiv_data/Learning to Fix Build Errors with Graph2Diff Neural Networks", "ref1": null, "ref3": null, "ref24": "arxiv_data/Sequence to Sequence Learning with Neural Networks", "ref20": null, "ref21": "arxiv_data/Inferring Javascript types using Graph Neural Networks", "ref23": null, "ref28": null, "ref8": null, "ref10": null, "ref11": null, "ref12": "arxiv_data/Learning to Transduce with Unbounded Memory", "ref14": "arxiv_data/Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "ref15": "arxiv_data/Neural GPUs Learn Algorithms", "ref19": "arxiv_data/Neural Programmer-Interpreters", "ref7": "arxiv_data/Making Neural Programming Architectures Generalize via Recursion", "ref9": "arxiv_data/Differentiable Programs with Neural Libraries", "ref4": null, "ref22": "arxiv_data/Learning Execution through Neural Code Fusion", "ref16": "arxiv_data/Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples", "ref18": "arxiv_data/Gated Graph Sequence Neural Networks", "ref17": null}, "arxiv_data/Learning Transfers over Several Programming Languages/2310.16937v1.Learning_Transfers_over_Several_Programming_Languages.cermxml": {"ref10": null, "ref18": "arxiv_data/CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation", "ref20": null, "ref27": "arxiv_data/Improving Code Autocompletion with Transfer Learning", "ref15": null, "ref9": null, "ref1": "arxiv_data/Using Document Similarity Methods to create Parallel Datasets for Code Translation", "ref24": "arxiv_data/Unsupervised Translation of Programming Languages", "ref2": "arxiv_data/Multilingual training for Software Engineering", "ref21": null, "ref26": null, "ref6": "arxiv_data/On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages", "ref16": null, "ref13": "arxiv_data/From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers", "ref8": null, "ref3": "arxiv_data/Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models", "ref22": null, "ref12": null, "ref5": null, "ref25": "arxiv_data/CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "ref14": null, "ref23": null, "ref17": "arxiv_data/CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation", "ref11": null, "ref4": null, "ref19": null}, "arxiv_data/Improving the Diproche CNL through autoformalization via GPT-3/2303.17513v1.Improving_the_Diproche_CNL_through_autoformalization_via_GPT_3.cermxml": {"ref2": null, "ref9": "arxiv_data/Autoformalization with Large Language Models", "ref6": "arxiv_data/Number Theory and Axiomatic Geometry in the Diproche System", "ref7": null, "ref3": null, "ref8": "arxiv_data/Exploration of Neural Machine Translation in Autoformalization of Mathematics in Mizar"}, "arxiv_data/Using Document Similarity Methods to create Parallel Datasets for Code Translation/2110.05423v1.Using_Document_Similarity_Methods_to_create_Parallel_Datasets_for_Code_Translation.cermxml": {"ref2": "arxiv_data/Unified Pre-training for Program Understanding and Generation", "ref3": "arxiv_data/AVATAR: A Parallel Corpus for Java-Python Program Translation", "ref28": null, "ref31": null, "ref33": null, "ref10": "arxiv_data/Language Models are Few-Shot Learners", "ref12": "arxiv_data/Evaluating Large Language Models Trained on Code", "ref21": "arxiv_data/Measuring Coding Challenge Competence With APPS", "ref38": null, "ref35": null, "ref9": null, "ref15": null, "ref29": null, "ref37": null, "ref32": "arxiv_data/CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks", "ref17": null, "ref41": "arxiv_data/Attention Is All You Need", "ref16": null, "ref14": null, "ref20": null, "ref34": "arxiv_data/CodeBLEU: a Method for Automatic Evaluation of Code Synthesis", "ref23": "arxiv_data/CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "ref30": null, "ref4": "arxiv_data/A Survey of Machine Learning for Big Code and Naturalness", "ref6": "arxiv_data/Program Synthesis with Large Language Models", "ref36": "arxiv_data/Unsupervised Translation of Programming Languages", "ref8": null}, "arxiv_data/Using Automated Theorem Provers for Mistake Diagnosis in the Didactics of Mathematics/2002.05083v1.Using_Automated_Theorem_Provers_for_Mistake_Diagnosis_in_the_Didactics_of_Mathematics.cermxml": {"ref3": null}, "arxiv_data/Improving Code Autocompletion with Transfer Learning/2105.05991v2.Improving_Code_Autocompletion_with_Transfer_Learning.cermxml": {"ref2": null, "ref3": null, "ref6": "arxiv_data/Code Prediction by Feeding Trees to Transformers", "ref7": "arxiv_data/Code Completion with Neural Attention and Pointer Networks", "ref1": "arxiv_data/Learning Autocompletion from Real-World Datasets", "ref8": null, "ref9": null, "ref13": null, "ref11": null, "ref14": "arxiv_data/The Adverse Effects of Code Duplication in Machine Learning Models of Code", "ref15": null, "ref16": "arxiv_data/Maybe Deep Neural Networks are the Best Choice for Modeling Source Code", "ref17": "arxiv_data/Neural Machine Translation of Rare Words with Subword Units", "ref18": "arxiv_data/A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code", "ref4": null, "ref19": null, "ref20": null, "ref21": null, "ref22": "arxiv_data/Generative Code Modeling with Graphs", "ref5": "arxiv_data/Sequence Model Design for Code Completion in the Modern IDE", "ref10": "arxiv_data/BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "ref23": "arxiv_data/RoBERTa: A Robustly Optimized BERT Pretraining Approach", "ref12": null, "ref24": "arxiv_data/Applying CodeBERT for Automated Program Repair of Java Simple Bugs", "ref30": null, "ref31": "arxiv_data/The Natural Language Decathlon: Multitask Learning as Question Answering", "ref27": "arxiv_data/Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks", "ref32": "arxiv_data/Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "ref25": "arxiv_data/Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models", "ref26": null, "ref28": null}, "arxiv_data/CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation/2109.00859v1.CodeT5__Identifier_aware_Unified_Pre_trained_Encoder_Decoder_Models_for_Code_Understanding_and_Generation.cermxml": {"ref10": null, "ref14": "arxiv_data/Learning and Evaluating Contextual Embedding of Source Code", "ref30": "arxiv_data/IntelliCode Compose: Code Generation Using Transformer", "ref23": "arxiv_data/Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "ref6": null, "ref12": "arxiv_data/CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "ref11": null, "ref20": null, "ref32": "arxiv_data/Attention Is All You Need", "ref7": null, "ref18": null, "ref19": "arxiv_data/RoBERTa: A Robustly Optimized BERT Pretraining Approach", "ref34": null, "ref3": null, "ref22": null, "ref28": null, "ref15": null, "ref8": null, "ref21": "arxiv_data/Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks", "ref4": null, "ref9": null, "ref1": "arxiv_data/Unified Pre-training for Program Understanding and Generation", "ref29": null, "ref27": "arxiv_data/Neural Machine Translation of Rare Words with Subword Units", "ref16": null, "ref13": "arxiv_data/Mapping Language to Code in Programmatic Context", "ref24": null, "ref2": null}, "arxiv_data/CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation/2102.04664v2.CodeXGLUE__A_Machine_Learning_Benchmark_Dataset_for_Code_Understanding_and_Generation.cermxml": {"ref23": null, "ref35": "arxiv_data/CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "ref58": null, "ref85": "arxiv_data/TranS^3: A Transformer-based Framework for Unifying Code Summarization and Code Search", "ref31": null, "ref4": null, "ref62": null, "ref63": null, "ref72": "arxiv_data/IntelliCode Compose: Code Generation Using Transformer", "ref73": null, "ref8": null, "ref9": null, "ref11": "arxiv_data/Tree-to-tree Neural Networks for Program Translation", "ref41": null, "ref46": "arxiv_data/Unsupervised Translation of Programming Languages", "ref54": null, "ref16": "arxiv_data/BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "ref69": "arxiv_data/Release Strategies and the Social Impacts of Language Models", "ref18": "arxiv_data/CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "ref15": null, "ref81": "arxiv_data/GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "ref10": null, "ref52": "arxiv_data/Convolutional Neural Networks over Tree Structures for Programming Language Processing", "ref71": null, "ref84": null, "ref89": null, "ref93": "arxiv_data/MISIM: A Neural Code Semantics Similarity System Using the Context-Aware Semantics Structure", "ref97": null, "ref47": null, "ref57": null, "ref61": null, "ref82": null, "ref83": null, "ref99": null, "ref2": "arxiv_data/Learning to Represent Programs with Graphs", "ref28": null, "ref30": null, "ref75": "arxiv_data/An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation", "ref76": "arxiv_data/Neural Program Repair by Jointly Learning to Localize and Repair", "ref78": "arxiv_data/Refinement Types for TypeScript", "ref12": "arxiv_data/PyMT5: multi-mode translation of natural language and Python code with transformers", "ref26": "arxiv_data/Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing", "ref36": "arxiv_data/Learning Programmatic Idioms for Scalable Semantic Parsing", "ref38": "arxiv_data/Mapping Language to Code in Programmatic Context", "ref87": "arxiv_data/Code Generation as a Dual Task of Code Summarization", "ref90": "arxiv_data/Incorporating External Knowledge through Pre-training for Natural Language to Code Generation", "ref94": "arxiv_data/Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow", "ref95": "arxiv_data/A Syntactic Neural Model for General-Purpose Code Generation", "ref19": "arxiv_data/Structured Neural Summarization", "ref3": "arxiv_data/A Convolutional Attention Network for Extreme Summarization of Source Code", "ref34": null, "ref37": null, "ref80": "arxiv_data/Improving Automatic Source Code Summarization via Deep Reinforcement Learning", "ref86": null, "ref40": null, "ref29": null, "ref42": "arxiv_data/Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code", "ref25": "arxiv_data/GraphCodeBERT: Pre-training Code Representations with Data Flow", "ref50": "arxiv_data/RoBERTa: A Robustly Optimized BERT Pretraining Approach", "ref91": null, "ref92": null, "ref21": null, "ref17": null, "ref59": null, "ref67": "arxiv_data/Neural Machine Translation of Rare Words with Subword Units", "ref39": null, "ref14": null, "ref88": null, "ref96": null, "ref6": null, "ref7": null, "ref51": null, "ref32": null, "ref43": "arxiv_data/Convolutional Neural Networks for Sentence Classification", "ref56": null, "ref65": "arxiv_data/CodeBLEU: a Method for Automatic Evaluation of Code Synthesis", "ref70": "arxiv_data/Sequence to Sequence Learning with Neural Networks", "ref44": null, "ref49": null, "ref66": "arxiv_data/Neural Machine Translation of Rare Words with Subword Units", "ref13": "arxiv_data/Unsupervised Cross-lingual Representation Learning at Scale", "ref55": null, "ref60": "arxiv_data/SQuAD: 100,000+ Questions for Machine Comprehension of Text", "ref33": "arxiv_data/XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization", "ref48": "arxiv_data/XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation", "ref1": "arxiv_data/A Survey of Machine Learning for Big Code and Naturalness", "ref5": "arxiv_data/Mining Idioms from Source Code", "ref27": null, "ref22": null, "ref74": "arxiv_data/Unit Test Case Generation with Transformers and Focal Context", "ref20": null, "ref45": null, "ref53": "arxiv_data/Neural Programmer: Inducing Latent Programs with Gradient Descent", "ref64": "arxiv_data/Neural Programmer-Interpreters", "ref68": null, "ref79": null, "ref98": null, "ref24": null}, "arxiv_data/The Pile: An 800GB Dataset of Diverse Text for Language Modeling/2101.00027v1.The_Pile__An_800GB_Dataset_of_Diverse_Text_for_Language_Modeling.cermxml": {"ref16": null, "ref48": null, "ref50": "arxiv_data/Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "ref56": null, "ref19": "arxiv_data/Extracting Training Data from Large Language Models", "ref58": "arxiv_data/Analysing Mathematical Reasoning Abilities of Neural Models", "ref63": null, "ref64": null, "ref49": "arxiv_data/Compressive Transformers for Long-Range Sequence Modelling", "ref67": null, "ref33": null, "ref30": null, "ref26": null, "ref60": null, "ref35": "arxiv_data/GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "ref69": null, "ref65": null, "ref23": null, "ref71": null, "ref28": null, "ref13": "arxiv_data/Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "ref29": null, "ref76": null, "ref11": "arxiv_data/Pitfalls in Machine Learning Research: Reexamining the Development Cycle", "ref25": "arxiv_data/AI Research Considerations for Human Existential Safety (ARCHES)", "ref47": null, "ref59": "arxiv_data/Baselines and a datasheet for the Cerema AWP dataset", "ref68": "arxiv_data/A Simple Method for Commonsense Reasoning", "ref7": null, "ref24": null, "ref9": null, "ref55": null, "ref39": null, "ref6": null, "ref43": null, "ref15": null, "ref57": null, "ref21": null, "ref62": null, "ref42": "arxiv_data/Distributed Representations of Words and Phrases and their Compositionality", "ref41": null, "ref54": null}, "arxiv_data/Neural Network Acceptability Judgments/1805.12471v3.Neural_Network_Acceptability_Judgments.cermxml": {"ref2": "arxiv_data/Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "ref25": "arxiv_data/Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "ref26": null, "ref28": "arxiv_data/Refining Targeted Syntactic Evaluation of Language Models", "ref46": null, "ref45": null, "ref23": null, "ref44": null, "ref16": null, "ref22": null, "ref27": null, "ref41": null, "ref42": null, "ref43": null, "ref47": "arxiv_data/Structural Supervision Improves Learning of Non-Local Grammatical Dependencies", "ref14": null, "ref19": null, "ref24": null, "ref34": null, "ref29": null, "ref15": "arxiv_data/Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "ref21": null, "ref32": "arxiv_data/Deep contextualized word representations", "ref6": null, "ref20": null, "ref31": null, "ref39": null, "ref10": null, "ref33": null, "ref13": null}, "arxiv_data/Multilingual training for Software Engineering/2112.02043v4.Multilingual_training_for_Software_Engineering.cermxml": {"ref16": "arxiv_data/A Comprehensive Survey of Multilingual Neural Machine Translation", "ref23": "arxiv_data/Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder", "ref57": "arxiv_data/Neural Machine Translation for Low-Resource Languages: A Survey", "ref63": "arxiv_data/Multilingual Translation with Extensible Multilingual Pretraining and Finetuning", "ref36": "arxiv_data/A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation", "ref62": "arxiv_data/Multilingual Neural Machine Translation with Knowledge Distillation", "ref9": "arxiv_data/Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges", "ref34": null, "ref20": null, "ref76": "arxiv_data/Multi-Source Neural Translation", "ref21": null, "ref30": null, "ref26": null, "ref40": "arxiv_data/A Neural Model for Generating Natural Language Summaries of Program Subroutines", "ref18": null, "ref74": null, "ref42": null, "ref1": "arxiv_data/Unified Pre-training for Program Understanding and Generation", "ref54": "arxiv_data/CoTexT: Multi-task Learning with Code-Text Transformer", "ref55": "arxiv_data/ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation", "ref2": "arxiv_data/A Transformer-based Approach for Source Code Summarization", "ref7": "arxiv_data/code2seq: Generating Sequences from Structured Representations of Code", "ref71": "arxiv_data/Retrieve and Refine: Exemplar-based Neural Comment Generation", "ref19": "arxiv_data/Code Structure Guided Transformer for Source Code Summarization", "ref24": "arxiv_data/Improved Automatic Summarization of Subroutines via Attention to File Context", "ref25": "arxiv_data/CoDesc: A Large Code-Description Parallel Dataset", "ref27": null, "ref28": null, "ref38": "arxiv_data/Ensemble Models for Neural Source Code Summarization of Subroutines", "ref39": "arxiv_data/Improved Code Summarization via a Graph Neural Network", "ref41": null, "ref49": "arxiv_data/Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors", "ref50": "arxiv_data/Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks", "ref66": "arxiv_data/Improving Automatic Source Code Summarization via Deep Reinforcement Learning", "ref67": null, "ref69": "arxiv_data/CoCoSum: Contextual Code Summarization with Multi-Relational Graph Neural Network", "ref70": "arxiv_data/Code Generation as a Dual Task of Code Summarization", "ref72": "arxiv_data/ComFormer: Code Comment Generation via Transformer and Fusion Method-based Hybrid Code Representation", "ref73": "arxiv_data/Yet Another Combination of IR- and Neural-based Comment Generation", "ref47": "arxiv_data/CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation", "ref51": null, "ref37": null, "ref12": "arxiv_data/On the Opportunities and Risks of Foundation Models", "ref22": null, "ref17": "arxiv_data/BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "ref45": "arxiv_data/RoBERTa: A Robustly Optimized BERT Pretraining Approach", "ref11": null, "ref3": "arxiv_data/Learning to Find Usages of Library Functions in Optimized Binaries", "ref32": null, "ref33": null, "ref35": "arxiv_data/Learning and Evaluating Contextual Embedding of Source Code", "ref4": "arxiv_data/SYNFIX: Automatically Fixing Syntax Errors using Compiler Diagnostics", "ref75": null, "ref65": null, "ref31": null, "ref13": "arxiv_data/Language Models are Few-Shot Learners", "ref15": null, "ref56": "arxiv_data/Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "ref64": "arxiv_data/Attention Is All You Need", "ref29": "arxiv_data/CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "ref14": "arxiv_data/ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "ref60": null, "ref5": "arxiv_data/The Adverse Effects of Code Duplication in Machine Learning Models of Code", "ref6": "arxiv_data/A Convolutional Attention Network for Extreme Summarization of Source Code", "ref44": null, "ref59": "arxiv_data/Neural Machine Translation of Rare Words with Subword Units", "ref8": null, "ref61": "arxiv_data/Sequence to Sequence Learning with Neural Networks", "ref58": null, "ref48": null, "ref46": null, "ref53": null, "ref43": null, "ref10": null, "ref52": "arxiv_data/Retrieval Augmented Code Generation and Summarization", "ref68": "arxiv_data/SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation"}, "arxiv_data/A Primer in BERTology: What we know about how BERT works/2002.12327v3.A_Primer_in_BERTology__What_we_know_about_how_BERT_works.cermxml": {"ref141": "arxiv_data/Attention Is All You Need", "ref157": "arxiv_data/Cross-Lingual Ability of Multilingual BERT: An Empirical Study", "ref32": null, "ref165": "arxiv_data/Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "ref113": "arxiv_data/Movement Pruning: Adaptive Sparsity by Fine-Tuning", "ref162": "arxiv_data/HuggingFace's Transformers: State-of-the-art Natural Language Processing", "ref45": null, "ref66": "arxiv_data/A Mutual Information Maximization Perspective of Language Representation Learning", "ref25": "arxiv_data/Unsupervised Cross-lingual Representation Learning at Scale", "ref107": "arxiv_data/What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge", "ref110": "arxiv_data/Inducing Syntactic Trees from BERT Representations", "ref111": "arxiv_data/DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "ref112": "arxiv_data/DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "ref12": "arxiv_data/Analysis Methods in Neural Language Processing: A Survey", "ref122": "arxiv_data/BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning", "ref123": "arxiv_data/Energy and Policy Considerations for Deep Learning in NLP", "ref124": "arxiv_data/SesameBERT: Attention for Anywhere", "ref126": "arxiv_data/Patient Knowledge Distillation for BERT Model Compression", "ref131": "arxiv_data/oLMpics -- On what Language Model Pre-training Captures", "ref134": "arxiv_data/BERT Rediscovers the Classical NLP Pipeline", "ref140": "arxiv_data/Quantity doesn't buy quality syntax with neural language models", "ref143": "arxiv_data/Analyzing the Structure of Attention in a Transformer Language Model", "ref144": "arxiv_data/Parsing as Pretraining", "ref145": "arxiv_data/The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives", "ref15": "arxiv_data/Inducing Relational Knowledge from BERT", "ref160": "arxiv_data/Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings", "ref161": "arxiv_data/Attention is not Explanation", "ref168": "arxiv_data/Deepening Hidden Representations from Pre-trained Language Models", "ref174": null, "ref178": null, "ref180": "arxiv_data/Improving BERT Fine-tuning with Embedding Normalization", "ref2": null, "ref22": "arxiv_data/What Does BERT Look At? An Analysis of BERT's Attention", "ref24": "arxiv_data/On the use of BERT for Neural Machine Translation", "ref26": "arxiv_data/Adaptively Sparse Transformers", "ref3": "arxiv_data/Transfer Fine-Tuning: A BERT Case Study", "ref30": "arxiv_data/Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations", "ref31": "arxiv_data/Commonsense Knowledge Mining from Pretrained Models", "ref37": "arxiv_data/Reducing Transformer Depth on Demand with Structured Dropout", "ref38": null, "ref4": null, "ref5": "arxiv_data/How Language-Neutral is Multilingual BERT?", "ref50": "arxiv_data/Visualizing and Understanding the Effectiveness of BERT", "ref52": "arxiv_data/exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models", "ref55": "arxiv_data/Attention is not Explanation", "ref63": "arxiv_data/Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction", "ref65": null, "ref70": "arxiv_data/Cross-lingual Language Model Pretraining", "ref73": null, "ref77": null, "ref85": "arxiv_data/Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference", "ref87": "arxiv_data/Are Sixteen Heads Really Better than One?", "ref88": "arxiv_data/What do you mean, BERT? Assessing BERT as a Distributional Semantics Model", "ref92": "arxiv_data/Probing Neural Network Comprehension of Natural Language Arguments", "ref94": "arxiv_data/To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks", "ref96": "arxiv_data/Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks", "ref98": null, "ref9": "arxiv_data/Segatron: Segment-Aware Transformer for Language Modeling and Understanding", "ref142": "arxiv_data/Visualizing Attention in Transformer-Based Language Representation Models", "ref16": "arxiv_data/Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking", "ref35": "arxiv_data/How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings", "ref84": null, "ref148": "arxiv_data/Universal Adversarial Triggers for Attacking and Analyzing NLP", "ref57": "arxiv_data/SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization", "ref58": "arxiv_data/How Can We Know What Language Models Know?", "ref135": "arxiv_data/What do you learn from context? Probing for sentence structure in contextualized word representations", "ref36": "arxiv_data/What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models", "ref133": "arxiv_data/Distilling Task-Specific Knowledge from BERT into Simple Neural Networks", "ref176": null, "ref29": "arxiv_data/Pre-Training with Whole Word Masking for Chinese BERT", "ref48": "arxiv_data/Reweighted Proximal Pruning for Large-Scale Language Representation", "ref74": null, "ref78": "arxiv_data/Linguistic Knowledge and Transferability of Contextual Representations", "ref79": "arxiv_data/RoBERTa: A Robustly Optimized BERT Pretraining Approach", "ref8": null, "ref159": "arxiv_data/Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs", "ref54": null, "ref100": "arxiv_data/Improving Transformer Models by Reordering their Sublayers", "ref103": "arxiv_data/Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation", "ref108": "arxiv_data/How Much Knowledge Can You Pack Into the Parameters of a Language Model?", "ref109": null, "ref114": null, "ref115": "arxiv_data/BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward", "ref125": "arxiv_data/Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets", "ref13": "arxiv_data/PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models", "ref14": null, "ref147": "arxiv_data/Information-Theoretic Probing with Minimum Description Length", "ref156": "arxiv_data/How Can BERT Help Lexical Semantics Tasks?", "ref158": "arxiv_data/Can neural networks acquire a structural bias from raw linguistic data?", "ref166": "arxiv_data/Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT", "ref173": "arxiv_data/GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference", "ref179": null, "ref181": null, "ref28": null, "ref34": "arxiv_data/Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals", "ref40": null, "ref42": "arxiv_data/Is Supervised Syntactic Parsing Beneficial for Language Understanding? An Empirical Investigation", "ref46": "arxiv_data/Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "ref6": "arxiv_data/On the comparability of Pre-trained Language Models", "ref64": "arxiv_data/Attention is Not Only a Weight: Analyzing Transformers with Vector Norms", "ref68": "arxiv_data/Data Augmentation using Pre-trained Transformer Models", "ref69": "arxiv_data/A Matter of Framing: The Impact of Linguistic Formalism on Probing Results", "ref75": "arxiv_data/Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering", "ref83": "arxiv_data/Structured Pruning of a BERT-based Question Answering Model", "ref86": null, "ref99": "arxiv_data/When BERT Plays the Lottery, All Tickets Are Winning", "ref90": "arxiv_data/Distributed Representations of Words and Phrases and their Compositionality", "ref82": "arxiv_data/On Measuring Social Biases in Sentence Encoders", "ref151": "arxiv_data/K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters", "ref154": "arxiv_data/MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers", "ref155": "arxiv_data/KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation", "ref104": null, "ref91": null, "ref132": null, "ref80": "arxiv_data/Universal Text Representation from BERT: An Empirical Study", "ref81": null, "ref116": "arxiv_data/Green AI", "ref19": "arxiv_data/On Identifiability in Transformers", "ref150": null, "ref43": null, "ref61": null, "ref127": "arxiv_data/ERNIE: Enhanced Representation through Knowledge Integration", "ref128": "arxiv_data/ERNIE 2.0: A Continual Pre-training Framework for Language Understanding", "ref172": "arxiv_data/Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "ref119": null, "ref120": "arxiv_data/What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?", "ref130": "arxiv_data/Syntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding", "ref152": "arxiv_data/StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "ref163": "arxiv_data/Pay Less Attention with Lightweight and Dynamic Convolutions", "ref177": "arxiv_data/Semantics-aware BERT for Language Understanding", "ref169": "arxiv_data/XLNet: Generalized Autoregressive Pretraining for Language Understanding", "ref182": "arxiv_data/FreeLB: Enhanced Adversarial Training for Natural Language Understanding", "ref21": "arxiv_data/Symmetric Regularization based BERT for Pair-wise Semantic Reasoning", "ref101": null, "ref139": null, "ref93": "arxiv_data/Knowledge Enhanced Contextual Word Representations", "ref27": null, "ref89": null, "ref164": null, "ref62": null, "ref7": null, "ref44": "arxiv_data/Assessing BERT's Syntactic Abilities", "ref51": null, "ref59": null, "ref71": "arxiv_data/ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "ref72": "arxiv_data/ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "ref121": "arxiv_data/MPNet: Masked and Permuted Pre-training for Language Understanding", "ref167": null, "ref17": null, "ref171": "arxiv_data/Learning and Evaluating General Linguistic Intelligence", "ref60": "arxiv_data/Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment", "ref67": null, "ref105": "arxiv_data/Beyond Accuracy: Behavioral Testing of NLP models with CheckList"}, "arxiv_data/Number Theory and Axiomatic Geometry in the Diproche System/2006.01794v3.Number_Theory_and_Axiomatic_Geometry_in_the_Diproche_System.cermxml": {"ref1": null, "ref2": "arxiv_data/Towards Intuitive Reasoning in Axiomatic Geometry", "ref3": null, "ref4": "arxiv_data/Using Automated Theorem Provers for Mistake Diagnosis in the Didactics of Mathematics", "ref5": "arxiv_data/Automatized Evaluation of Formalization Exercises in Mathematics", "ref6": null, "ref7": null, "ref8": null, "ref9": null}, "arxiv_data/Learning in High Dimension Always Amounts to Extrapolation/2110.09485v2.Learning_in_High_Dimension_Always_Amounts_to_Extrapolation.cermxml": {"ref3": null, "ref6": null, "ref19": null, "ref25": null, "ref11": null, "ref13": null, "ref5": null, "ref2": null, "ref23": null, "ref20": "arxiv_data/Smoothed Analysis of Algorithms: Why the Simplex Algorithm Usually Takes Polynomial Time", "ref26": null, "ref17": null, "ref14": null, "ref21": null, "ref9": null, "ref15": "arxiv_data/Absorption probabilities for Gaussian polytopes and regular spherical simplices", "ref18": "arxiv_data/Random Convex Hulls and Extreme Value Statistics"}, "arxiv_data/Lost in Translation: Large Language Models in Non-English Content Analysis/2306.07377v1.Lost_in_Translation__Large_Language_Models_in_Non_English_Content_Analysis.cermxml": {"ref58": null, "ref10": "arxiv_data/Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond", "ref41": null, "ref48": null, "ref62": "arxiv_data/A New Generation of Perspective API: Efficient Multilingual Character-level Transformers", "ref36": null, "ref56": null, "ref40": null, "ref49": null, "ref74": null, "ref72": null, "ref22": null, "ref4": null, "ref15": null, "ref38": null, "ref30": null, "ref52": "arxiv_data/How to Train BERT with an Academic Budget", "ref21": null, "ref35": null, "ref88": null, "ref6": null, "ref53": "arxiv_data/The State and Fate of Linguistic Diversity and Inclusion in the NLP World", "ref13": null, "ref1": null, "ref2": null, "ref16": "arxiv_data/Towards Responsible Natural Language Annotation for the Varieties of Arabic", "ref66": null, "ref39": "arxiv_data/Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus", "ref59": "arxiv_data/Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets", "ref27": "arxiv_data/Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus", "ref75": "arxiv_data/Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages", "ref24": null, "ref65": null, "ref31": null, "ref76": null, "ref23": null, "ref79": null, "ref3": "arxiv_data/Give your Text Representation Models some Love: the Case for Basque", "ref37": null, "ref68": null, "ref69": null, "ref7": null, "ref71": "arxiv_data/Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese", "ref83": null, "ref46": "arxiv_data/Predictability and Surprise in Large Generative Models", "ref34": "arxiv_data/Emerging Cross-lingual Structure in Pretrained Language Models", "ref12": null, "ref47": null, "ref5": null, "ref87": null, "ref64": null}, "arxiv_data/How Good are Commercial Large Language Models on African Languages?/2305.06530v1.How_Good_are_Commercial_Large_Language_Models_on_African_Languages_.cermxml": {"ref12": null, "ref29": null, "ref40": "arxiv_data/Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "ref20": "arxiv_data/Universal Language Model Fine-tuning for Text Classification", "ref23": null, "ref10": null, "ref11": null, "ref41": "arxiv_data/Multitask Prompted Training Enables Zero-Shot Task Generalization", "ref42": null, "ref45": null, "ref46": "arxiv_data/Emergent Abilities of Large Language Models", "ref47": null, "ref53": "arxiv_data/Discrete and Soft Prompting for Multilingual Models", "ref6": null, "ref28": null, "ref51": "arxiv_data/GLM-130B: An Open Bilingual Pre-trained Model", "ref44": null, "ref37": "arxiv_data/XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "ref21": null, "ref52": null, "ref50": "arxiv_data/Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources", "ref2": "arxiv_data/Translation Artifacts in Cross-lingual Transfer Learning", "ref35": "arxiv_data/Training language models to follow instructions with human feedback", "ref15": null, "ref25": "arxiv_data/On Language Models for Creoles", "ref33": null, "ref9": null, "ref36": null, "ref38": "arxiv_data/A Call for Clarity in Reporting BLEU Scores", "ref27": "arxiv_data/Few-shot Learning with Multilingual Language Models", "ref34": null, "ref43": null, "ref49": "arxiv_data/Language Models are Few-shot Multilingual Learners", "ref18": "arxiv_data/The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "ref24": "arxiv_data/The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "ref48": "arxiv_data/CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data", "ref3": null, "ref7": "arxiv_data/It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information", "ref4": "arxiv_data/Systematic Inequalities in Language Technology Performance across the World's Languages", "ref1": null, "ref14": "arxiv_data/AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages", "ref32": "arxiv_data/No Language Left Behind: Scaling Human-Centered Machine Translation", "ref17": "arxiv_data/BLEU might be Guilty but References are not Innocent", "ref30": "arxiv_data/Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics", "ref8": null}, "arxiv_data/A Precis of Language Models are not Models of Language/2205.07634v1.A_Precis_of_Language_Models_are_not_Models_of_Language.cermxml": {"ref2": null, "ref1": null, "ref4": "arxiv_data/Neural Network Acceptability Judgments", "ref5": null, "ref6": null, "ref7": null, "ref8": null, "ref9": null, "ref10": null, "ref11": null, "ref12": null, "ref14": null, "ref15": "arxiv_data/Learning in High Dimension Always Amounts to Extrapolation", "ref16": null}, "arxiv_data/From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers/2005.00633v1.From_Zero_to_Hero__On_the_Limitations_of_Zero_Shot_Cross_Lingual_Transfer_with_Multilingual_Transformers.cermxml": {"ref12": null, "ref16": null, "ref45": null, "ref21": "arxiv_data/The State and Fate of Linguistic Diversity and Inclusion in the NLP World", "ref39": "arxiv_data/Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "ref5": null, "ref20": null, "ref26": "arxiv_data/Choosing Transfer Languages for Cross-Lingual Learning", "ref3": "arxiv_data/On the Cross-lingual Transferability of Monolingual Representations", "ref38": null, "ref4": "arxiv_data/Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond", "ref8": "arxiv_data/Multilingual Alignment of Contextual Word Representations", "ref18": "arxiv_data/How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions", "ref44": null, "ref10": null, "ref13": null, "ref40": "arxiv_data/Massively Multilingual Transfer for NER", "ref49": "arxiv_data/Energy and Policy Considerations for Deep Learning in NLP", "ref54": null, "ref56": null, "ref57": "arxiv_data/Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT", "ref9": null, "ref22": "arxiv_data/Cross-Lingual Ability of Multilingual BERT: An Empirical Study", "ref43": null, "ref52": null, "ref53": null, "ref27": null, "ref15": "arxiv_data/Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!", "ref30": null, "ref24": null, "ref35": null, "ref7": null, "ref31": null, "ref33": null, "ref41": null, "ref48": "arxiv_data/On the Limitations of Unsupervised Bilingual Dictionary Induction", "ref55": "arxiv_data/A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "ref1": "arxiv_data/Many Languages, One Parser", "ref2": null, "ref32": "arxiv_data/Exploiting Similarities among Languages for Machine Translation", "ref47": "arxiv_data/Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "ref51": "arxiv_data/Attention Is All You Need", "ref36": null, "ref11": null, "ref42": null, "ref14": "arxiv_data/Deep Biaffine Attention for Neural Dependency Parsing", "ref37": null, "ref6": null, "ref58": "arxiv_data/Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "ref46": "arxiv_data/Neural Machine Translation of Rare Words with Subword Units", "ref23": null}, "arxiv_data/Unsupervised Translation of Programming Languages/2006.03511v3.Unsupervised_Translation_of_Programming_Languages.cermxml": {"ref29": null, "ref21": null, "ref16": null, "ref1": null, "ref31": null, "ref30": null, "ref15": null, "ref2": "arxiv_data/Learning Natural Coding Conventions", "ref28": "arxiv_data/Code Completion with Neural Attention and Pointer Networks", "ref12": null, "ref40": "arxiv_data/Dynamic Neural Program Embedding for Program Repair", "ref10": null, "ref18": "arxiv_data/Towards Neural Decompilation", "ref17": null, "ref3": "arxiv_data/code2seq: Generating Sequences from Structured Representations of Code", "ref33": "arxiv_data/Abstract Syntax Networks for Code Generation and Semantic Parsing", "ref4": "arxiv_data/Structural Language Models of Code", "ref41": "arxiv_data/A Syntactic Neural Model for General-Purpose Code Generation", "ref5": "arxiv_data/Neural Attribute Machines for Program Generation", "ref13": "arxiv_data/The FLoRes Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English", "ref11": "arxiv_data/On Using Monolingual Corpora in Neural Machine Translation", "ref14": "arxiv_data/Dual Learning for Machine Translation", "ref34": "arxiv_data/Improving Neural Machine Translation Models with Monolingual Data", "ref42": null, "ref24": "arxiv_data/Unsupervised Machine Translation Using Monolingual Corpora Only", "ref8": "arxiv_data/Unsupervised Neural Machine Translation", "ref26": null, "ref7": "arxiv_data/Unsupervised Statistical Machine Translation", "ref37": "arxiv_data/Sequence to Sequence Learning with Neural Networks", "ref9": "arxiv_data/Neural Machine Translation by Jointly Learning to Align and Translate", "ref38": "arxiv_data/Attention Is All You Need", "ref25": "arxiv_data/Word Translation Without Parallel Data", "ref6": null, "ref23": "arxiv_data/Cross-lingual Language Model Pretraining", "ref27": "arxiv_data/BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "ref36": null, "ref39": null, "ref19": null, "ref32": null, "ref22": "arxiv_data/SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing", "ref35": "arxiv_data/Neural Machine Translation of Rare Words with Subword Units", "ref20": null}, "arxiv_data/Cedille: A large autoregressive French language model/2202.03371v1.Cedille__A_large_autoregressive_French_language_model.cermxml": {"ref1": null, "ref2": null, "ref3": null}}